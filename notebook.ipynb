{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based and model-free Reinforcement Learning on Gridworld Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are showing 4 methods here:\n",
    "1. Value Iteration - Model-based algorithm\n",
    "2. Policy Iteration - Model-based algorithm\n",
    "3. Temporal Difference Learning(1 step look-ahead) - Model-free algorithm\n",
    "4. Temporal Difference-lambda Learning(n step look-ahead) - Model-free algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration: GridWorld : Top-left and bottom-right corners are the terminal state\n",
      "No of rows: 4\n",
      "No of cols: 4\n",
      "Threshold for value function convergence0.00001\n",
      "Value function has converged \n",
      "\n",
      "Policy: \n",
      "['X', 'left', 'left', ['down/', 'left/']]\n",
      "['up', ['up/', 'left/'], ['up/', 'down/', 'right/', 'left/'], 'down']\n",
      "['up', ['up/', 'down/', 'right/', 'left/'], ['down/', 'right/'], 'down']\n",
      "[['up/', 'right/'], 'right', 'right', 'X']\n",
      "\n",
      "\n",
      "Do note that for some states, the policy might not have converged due to a high threshold, or there might be more than one optimal action for that state\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This file demonstrates Value Iteration on the GridWorld problem\n",
    "Falls under: Model-based algorithms\n",
    "Are state transitions stochastic: No (deterministic upon taking an action)\n",
    "Initial policy: Uniform random( other policies can be added later)\n",
    "Initial Value function: All 0s\n",
    "Reward function: -1 for every step taken( other reward functions can be added later). Also, the rewards are dependent\n",
    "                 only on the actions, not on the states\n",
    "Terminal state(s): Top left corner and bottom right. Other terminal states can be added by harding them in the\n",
    "                   following 2 places in the code:\n",
    "                   TODO: 1)\n",
    "                   TODO: 2)\n",
    "\n",
    "Note how the policy we arrive at is sensitive to the convergence value and the size of the grid. As the size of the grid\n",
    "increases, the convergence value needs to be made smaller to arrive at the true optimal policy. Otherwise, the\n",
    "iterations end at a stage where there is more than one optimal action for some state( one of them might not actually be\n",
    "optimal, its just that we converged, rather, stopped evaluating too early to realize that it wasn't an optimal action\n",
    "for that state)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"Value iteration: GridWorld : Top-left and bottom-right corners are the terminal state\")\n",
    "\n",
    "n = int(input(\"No of rows: \"))\n",
    "m = int(input(\"No of cols: \"))\n",
    "\n",
    "threshold = float(input(\"Threshold for value function convergence\"))\n",
    "\n",
    "\"\"\"\n",
    "This function is used to modify the reward function.\n",
    "A simple reward function is -1 reward for any direction chosen.\n",
    "Modify the values from -1 to any other value for custom rewards for the directions\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_reward_function():\n",
    "    # Note: The actions are assumed to be in the format: [up, down, right, left]\n",
    "    reward = list()\n",
    "    reward.append(-1)  # up\n",
    "    reward.append(-1)  # down\n",
    "    reward.append(-1)  # right\n",
    "    reward.append(-1)  # left\n",
    "\n",
    "    reward = np.array(reward)\n",
    "    return reward\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function is used to calculate the expected value function\n",
    "given the set of rewards for every action, reward function and a policy.\n",
    "To account for the first iteration( our initial policy), the default policy is uniform random.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_optimal_value_fnc(values, reward, policy=None):\n",
    "    if policy is None:  # policy is uniform random\n",
    "        pdf = 0.25 * np.ones((4, ))\n",
    "        expected_value = pdf * (reward + values)\n",
    "        return np.max(expected_value)  # the optimal value function for this state is the max. of all the values of\n",
    "        #  states we might end up in from this state\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function gives the value of adjacent states for a given state\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_adjacent_indices(i, j, n, m):\n",
    "    positions = list()\n",
    "\n",
    "    # up\n",
    "    positions.append([i, j]) if i == 0 else positions.append([i - 1, j])\n",
    "\n",
    "    # down\n",
    "    positions.append([i, j]) if i == (n - 1) else positions.append([i + 1, j])\n",
    "\n",
    "    # right\n",
    "    positions.append([i, j]) if j == (m - 1) else positions.append([i, j + 1])\n",
    "\n",
    "    # left\n",
    "    positions.append([i, j]) if j == 0 else positions.append([i, j - 1])\n",
    "\n",
    "    positions = np.array(positions)\n",
    "    return positions\n",
    "\n",
    "\n",
    "grid_world = np.zeros((n, m))  # holds value function\n",
    "rewards = get_reward_function()\n",
    "\n",
    "# evaluate the value function ( 1st iteration)\n",
    "for i in range(n):\n",
    "    for j in range(m):\n",
    "        if i == 0 and j == 0:  # top-left corner of the grid\n",
    "            grid_world[i][j] = 0\n",
    "\n",
    "        else:\n",
    "            positions = get_adjacent_indices(i, j, n, m)\n",
    "\n",
    "            values = np.array(\n",
    "                [grid_world[positions[0][0]][positions[0][1]], grid_world[positions[1][0]][positions[1][1]],\n",
    "                 grid_world[positions[2][0]][positions[2][1]], grid_world[positions[3][0]][positions[3][1]]])\n",
    "\n",
    "            grid_world[i][j] = get_optimal_value_fnc(values, rewards)\n",
    "\n",
    "new_grid_world = np.zeros((n, m))\n",
    "\n",
    "diff = grid_world - new_grid_world\n",
    "mse = np.sqrt(np.sum(np.square(diff)))\n",
    "\n",
    "while mse > threshold:\n",
    "    new_grid_world = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            if i == 0 and j == 0:  # top-left corner of the grid\n",
    "                grid_world[i][j] = 0\n",
    "            elif i == (n - 1) and j == (m - 1):  # bottom-right corner of the grid\n",
    "                grid_world[i][j] = 0\n",
    "            # TODO: 1) add as many terminal states as you want here\n",
    "            else:\n",
    "                positions = get_adjacent_indices(i, j, n, m)\n",
    "\n",
    "                values = np.array([grid_world[positions[0][0]][positions[0][1]], grid_world[positions[1][0]][positions[1][1]],\n",
    "                                   grid_world[positions[2][0]][positions[2][1]], grid_world[positions[3][0]][positions[3][1]]])\n",
    "\n",
    "                new_grid_world[i][j] = get_optimal_value_fnc(values, rewards)\n",
    "\n",
    "    mse = np.sqrt(np.sum(np.square(grid_world - new_grid_world)))\n",
    "    grid_world = new_grid_world\n",
    "\n",
    "    # now our value function is ready, let's act greedily based on that and form a policy\n",
    "    # the initial policy is: go in any of the directions( though this is not explicitly mentioned)\n",
    "    # with iterations, we become more specific with which actions put us in a \"better\" state\n",
    "    # we stop evaluating the value function when we narrow down to one action for every state\n",
    "    # beyond this iteration, there is no point in improving the value function as the policy has converged.\n",
    "\n",
    "    # print(grid_world)\n",
    "\n",
    "print(\"Value function has converged \\n\")\n",
    "\n",
    "print(\"Policy: \")\n",
    "action_dictionary = {0: 'up', 1: 'down', 2: 'right', 3: 'left'}\n",
    "policy = []\n",
    "for i in range(n):\n",
    "    row_list = []\n",
    "    for j in range(m):\n",
    "        positions = get_adjacent_indices(i, j, n, m)\n",
    "\n",
    "        values = np.array([grid_world[positions[0][0]][positions[0][1]], grid_world[positions[1][0]][positions[1][1]],\n",
    "                           grid_world[positions[2][0]][positions[2][1]], grid_world[positions[3][0]][positions[3][1]]])\n",
    "\n",
    "        max_pos = np.argwhere(values == np.amax(values)).flatten()\n",
    "\n",
    "        if i == 0 and j == 0:  # top-left\n",
    "            row_list.append(\"X\")\n",
    "        elif i == (n - 1) and j == (m - 1):  # bottom-right\n",
    "            row_list.append(\"X\")\n",
    "        # TODO: 2) add as many terminal states as you want here\n",
    "        else:\n",
    "            if max_pos.shape[0] == 1:  # policy has converged for this state\n",
    "                row_list.append(action_dictionary[max_pos[0]])\n",
    "\n",
    "            else:\n",
    "                g = []\n",
    "                for t in range(max_pos.shape[0]):\n",
    "                    g.append(action_dictionary[max_pos[t]] + \"/\")\n",
    "                row_list.append(g)\n",
    "\n",
    "    policy.append(row_list)\n",
    "    print(row_list)\n",
    "\n",
    "# print(policy)\n",
    "\n",
    "print(\"\\n\\nDo note that for some states, the policy might not have converged due to a high threshold, or there might\"\n",
    "      \" be more than one optimal action for that state\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy iteration: GridWorld : Top-left and bottom-right corners are the terminal state\n",
      "No of rows: 4\n",
      "No of cols: 4\n",
      "Grid world policy function: \n",
      "[[ 0.     -2.4375 -2.9375 -3.    ]\n",
      " [-2.4375 -2.875  -3.     -2.9375]\n",
      " [-2.9375 -3.     -2.875  -2.4375]\n",
      " [-3.     -2.9375 -2.4375  0.    ]]\n",
      "\n",
      " Policy: \n",
      "['X', 'left', 'left', ['down/', 'left/']]\n",
      "['up', ['up/', 'left/'], ['down/', 'left/'], 'down']\n",
      "['up', ['up/', 'right/'], ['down/', 'right/'], 'down']\n",
      "[['up/', 'right/'], 'right', 'right', 'X']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This file demonstrates Policy Iteration on the GridWorld problem\n",
    "Falls under: Model-based algorithms\n",
    "Are state transitions stochastic: No (deterministic upon taking an action)\n",
    "Initial policy: Uniform random( other policies can be added later)\n",
    "Initial Value function: All 0s\n",
    "Reward function: -1 for every step taken( other reward functions can be added later). Also, the rewards are dependent\n",
    "                 only on the actions, not on the states\n",
    "Terminal state: Top left corner and bottom right. Other terminal states can be added by hard-coding them in the\n",
    "                   following 4 places in the code:\n",
    "                   TODO: 1)\n",
    "                   TODO: 2)\n",
    "                   TODO: 3)\n",
    "                   TODO: 4)\n",
    "                   Side note: Hard-coding is certainly not good design , but it's sufficient to demonstrate this simple\n",
    "                   problem\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"Policy iteration: GridWorld : Top-left and bottom-right corners are the terminal state\")\n",
    "\n",
    "n = int(input(\"No of rows: \"))\n",
    "m = int(input(\"No of cols: \"))\n",
    "\n",
    "\"\"\"\n",
    "This function is used to modify the reward function.\n",
    "A simple reward function is -1 reward for any direction chosen.\n",
    "Modify the values from -1 to any other value for custom rewards for the directions\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_reward_function():\n",
    "    # Note: The actions are assumed to be in the format: [up, down, right, left]\n",
    "    # modify the rewards if needed\n",
    "    reward = list()\n",
    "    reward.append(-1)  # up\n",
    "    reward.append(-1)  # down\n",
    "    reward.append(-1)  # right\n",
    "    reward.append(-1)  # left\n",
    "\n",
    "    reward = np.array(reward)\n",
    "    return reward\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function is used to calculate the expected value function\n",
    "given the set of rewards for every action, reward function and a policy.\n",
    "To account for the first iteration( our initial policy), the default policy is uniform random.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_expected_value_fnc(values, reward, policy=None):\n",
    "    if policy is None:  # policy is uniform random\n",
    "        pdf = 0.25 * np.ones((4, ))\n",
    "        expected_value = pdf * (reward + values)\n",
    "        return np.sum(expected_value)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function gives the value of adjacent states for a given state\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_adjacent_indices(i, j, n, m):\n",
    "    positions = list()\n",
    "\n",
    "    # up\n",
    "    positions.append([i, j]) if i == 0 else positions.append([i - 1, j])\n",
    "\n",
    "    # down\n",
    "    positions.append([i, j]) if i == (n - 1) else positions.append([i + 1, j])\n",
    "\n",
    "    # right\n",
    "    positions.append([i, j]) if j == (m - 1) else positions.append([i, j + 1])\n",
    "\n",
    "    # left\n",
    "    positions.append([i, j]) if j == 0 else positions.append([i, j - 1])\n",
    "\n",
    "    positions = np.array(positions)\n",
    "    return positions\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function compares the new policy and previous policy for every state.\n",
    "returns: True if there is a change in policy in at least one state\n",
    "         False if the 2 policies are exactly the same\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def has_changed(previous_policy, current_policy):\n",
    "    for x in range(len(previous_policy)):\n",
    "        for y in range(len(previous_policy[0])):\n",
    "            if not set(previous_policy[x][y]) == set(current_policy[x][y]):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "grid_world = np.zeros((n, m))  # holds value function\n",
    "rewards = get_reward_function()\n",
    "\n",
    "action_dictionary = {0: 'up', 1: 'down', 2: 'right', 3: 'left'}\n",
    "current_policy = []\n",
    "for i in range(n):\n",
    "    new_list = []\n",
    "    for j in range(m):\n",
    "        if i == 0 and j == 0:  # top-left corner of the grid\n",
    "            new_list.append([5])\n",
    "        elif i == (n - 1) and j == (m - 1):  # bottom-right corner of the grid\n",
    "            new_list.append([5])\n",
    "        # TODO: 1) add as many terminal states as you want here\n",
    "        else:\n",
    "            new_list.append([0, 1, 2, 3])\n",
    "\n",
    "    current_policy.append(new_list)\n",
    "\n",
    "new_policy = []\n",
    "while True:\n",
    "    new_grid_world = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            if i == 0 and j == 0:  # top-left corner of the grid\n",
    "                grid_world[i][j] = 0\n",
    "            elif i == (n - 1) and j == (m - 1):  # bottom-right corner of the grid\n",
    "                grid_world[i][j] = 0\n",
    "            # TODO: 2) add as many terminal states as you want here\n",
    "            else:\n",
    "                positions = get_adjacent_indices(i, j, n, m)\n",
    "\n",
    "                values = np.array([grid_world[positions[0][0]][positions[0][1]], grid_world[positions[1][0]][positions[1][1]],\n",
    "                                   grid_world[positions[2][0]][positions[2][1]], grid_world[positions[3][0]][positions[3][1]]])\n",
    "\n",
    "                new_grid_world[i][j] = get_expected_value_fnc(values, rewards)\n",
    "\n",
    "                # now our value function is ready, let's act greedily based on that and form a policy\n",
    "                # the initial policy is: go in any of the directions( though this is not explicitly mentioned)\n",
    "                # with iterations, we become more specific with which actions put us in a \"better\" state\n",
    "\n",
    "    new_policy = []\n",
    "    for i in range(n):\n",
    "        new_list = []\n",
    "        for j in range(m):\n",
    "            positions = get_adjacent_indices(i, j, n, m)\n",
    "\n",
    "            new_values = np.array(\n",
    "                [new_grid_world[positions[0][0]][positions[0][1]], new_grid_world[positions[1][0]][positions[1][1]],\n",
    "                 new_grid_world[positions[2][0]][positions[2][1]], new_grid_world[positions[3][0]][positions[3][1]]])\n",
    "\n",
    "            # print(\"new values from new gridworld:\", new_values, i, j)\n",
    "            max_pos = np.argwhere(new_values == np.amax(new_values)).flatten()\n",
    "            # print(\"max value from new gridworld:\", max_pos, i, j)\n",
    "\n",
    "            if i == 0 and j == 0:  # top-left corner of the grid\n",
    "                new_list.append([5])  # terminal states append 5\n",
    "            elif i == (n - 1) and j == (m - 1):  # bottom-right corner of the grid\n",
    "                new_list.append([5])  # terminal states append 5\n",
    "            # TODO: 3) add as many terminal states as you want here\n",
    "            else:\n",
    "                g = []\n",
    "                for t in range(max_pos.shape[0]):\n",
    "                    g.append(max_pos[t])\n",
    "                new_list.append(g)\n",
    "\n",
    "        new_policy.append(new_list)\n",
    "\n",
    "    # we stop evaluating the value function when we narrow down to one action for every state\n",
    "    # beyond this iteration, there is no point in improving the value function as the policy has converged.\n",
    "    # OR IN A GENERIC CASE\n",
    "    # when we see that the policy remains the same over 2 consecutive iterations.\n",
    "\n",
    "    # print(\"current_policy:\", current_policy)\n",
    "    # print(\"new_policy:\", new_policy)\n",
    "\n",
    "    if not has_changed(current_policy, new_policy):\n",
    "        break\n",
    "\n",
    "    grid_world = new_grid_world\n",
    "    current_policy = new_policy\n",
    "\n",
    "    # print(grid_world)\n",
    "    # print(\"-----------\")\n",
    "\n",
    "print(\"Grid world policy function: \")\n",
    "print(grid_world)\n",
    "\n",
    "print(\"\\n Policy: \")\n",
    "# POLICY\n",
    "policy = []\n",
    "for i in range(n):\n",
    "    row_list = []\n",
    "    for j in range(m):\n",
    "        positions = get_adjacent_indices(i, j, n, m)\n",
    "\n",
    "        values = np.array([grid_world[positions[0][0]][positions[0][1]], grid_world[positions[1][0]][positions[1][1]],\n",
    "                           grid_world[positions[2][0]][positions[2][1]], grid_world[positions[3][0]][positions[3][1]]])\n",
    "\n",
    "        max_pos = np.argwhere(values == np.amax(values)).flatten()\n",
    "\n",
    "        if i == 0 and j == 0:  # top-left\n",
    "            row_list.append(\"X\")\n",
    "        elif i == (n - 1) and j == (m - 1):  # bottom-right\n",
    "            row_list.append(\"X\")\n",
    "        # TODO: 4) add as many terminal states as you want here\n",
    "        else:\n",
    "            if max_pos.shape[0] == 1:  # policy has converged for this state\n",
    "                row_list.append(action_dictionary[max_pos[0]])\n",
    "\n",
    "            else:\n",
    "                g = []\n",
    "                for t in range(max_pos.shape[0]):\n",
    "                    g.append(action_dictionary[max_pos[t]] + \"/\")\n",
    "                row_list.append(g)\n",
    "\n",
    "    policy.append(row_list)\n",
    "    print(row_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the discount factor: 0.9\n",
      "Enter the update factor: 0.1\n",
      "How many episodes? 10\n",
      "Policy: \n",
      "up\n",
      "up\n",
      "up\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "down\n",
      "down\n",
      "down\n",
      "down\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "SARSA is the model free control technique of using TD(0) for policy evaluation and GLIE for policy improvement.\n",
    "In GLIE, the epsilon value( the value that controls the probability with which a random action is chosen v/s the greedy\n",
    "action is hyperbolically degraded as more episodes are seen. This ensures that we explore more initially and get greedy\n",
    "when we pick actions as we see more episodes. We can also use a constant epsilon and\n",
    "compare the 2 scenarios.\n",
    "\n",
    "It displays:\n",
    "1) The steps that need to be taken to reach the goal\n",
    "2) Graph of no of steps it took to complete the episode v/s episode no\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import environment\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sequence = []\n",
    "env = environment.GridWorld8x8()\n",
    "actions = [0, 1, 2, 3]\n",
    "iteration = 0\n",
    "\n",
    "# initializing q values\n",
    "q = np.zeros((env.m, env.n, len(env.actions)))\n",
    "\n",
    "discount_factor = float(input(\"Enter the discount factor: \"))  # gamma\n",
    "update_factor = float(input(\"Enter the update factor: \"))  # alpha\n",
    "\n",
    "episodes = int(input(\"How many episodes? \"))\n",
    "\n",
    "\n",
    "def get_action(rand_or_greedy, m, n):\n",
    "    if rand_or_greedy == 0:  # random\n",
    "        return random.choice(actions)\n",
    "\n",
    "    else:  # greedy\n",
    "        return np.argmax(q[m][n])\n",
    "\n",
    "\n",
    "steps = []\n",
    "for episode in range(episodes):\n",
    "    # defining epsilon: used to implement GLIE\n",
    "    # 0 means random, 1 means greedy action will be chosen.\n",
    "    # So, 0 -> epsilon 1 -> (1 - epsilon)\n",
    "\n",
    "    epsilon = 1 / (episode + 1)\n",
    "\n",
    "    current_state = env.starting_point\n",
    "\n",
    "    random_or_greedy = np.random.choice(np.arange(0, 2), p=[epsilon, (1 - epsilon)])\n",
    "    action = get_action(random_or_greedy, current_state[0], current_state[1])\n",
    "    step_count = 0\n",
    "\n",
    "    while True:\n",
    "        reward, next_state, terminal_state = env.take_action(current_state[0], current_state[1], action)\n",
    "\n",
    "        random_or_greedy = np.random.choice(np.arange(0, 2), p=[epsilon, (1 - epsilon)])\n",
    "        action_dash = get_action(random_or_greedy, next_state[0], next_state[1])\n",
    "\n",
    "        q[current_state[0]][current_state[1]][action] += update_factor * (reward + discount_factor *\n",
    "                        q[next_state[0]][next_state[1]][action_dash] - q[current_state[0]][current_state[1]][action])\n",
    "\n",
    "        current_state = next_state\n",
    "        action = action_dash\n",
    "\n",
    "        step_count += 1\n",
    "\n",
    "        if terminal_state:\n",
    "            steps.append(step_count)\n",
    "            break\n",
    "\n",
    "# POLICY:\n",
    "print(\"Policy: \")\n",
    "current_state = env.starting_point\n",
    "while True:\n",
    "    action = np.argmax(q[current_state[0]][current_state[1]])\n",
    "    # print(current_state)\n",
    "    if action == 0:\n",
    "        print(\"up\")\n",
    "    elif action == 1:\n",
    "        print(\"down\")\n",
    "    elif action == 2:\n",
    "        print(\"right\")\n",
    "    else:\n",
    "        print(\"left\")\n",
    "\n",
    "    reward, next_state, terminal_state = env.take_action(current_state[0], current_state[1], action)\n",
    "    current_state = next_state\n",
    "\n",
    "    if terminal_state:\n",
    "        break\n",
    "\n",
    "plt.plot(range(episodes), steps)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Steps taken to complete the episode\")\n",
    "plt.title(\"Steps taken per episode\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(lambda) Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the discount factor: 0.9\n",
      "Enter the update factor: 0.1\n",
      "Enter lambda, the factor that decays the eligibility traces: 0.1\n",
      "How many episodes? 10\n",
      "Policy: \n",
      "right\n",
      "right\n",
      "right\n",
      "down\n",
      "down\n",
      "down\n",
      "right\n",
      "right\n",
      "down\n",
      "down\n",
      "down\n",
      "down\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "SARSA(lam) is the model free control technique of using TD(lam) for policy evaluation and GLIE for policy improvement.\n",
    "In GLIE, the epsilon value( the value that controls the probability with which a random action is chosen v/s the greedy\n",
    "action is hyperbolically degraded as more episodes are seen.  This ensures that we explore more initially and get greedy\n",
    "when we pick actions as we see more episodes. We can also use a constant epsilon and\n",
    "compare the 2 scenarios.\n",
    "\n",
    "It displays:\n",
    "1) The steps that need to be taken to reach the goal\n",
    "2) Graph of no of steps it took to complete the episode v/s episode no\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import environment\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sequence = []\n",
    "env = environment.GridWorld8x8()\n",
    "actions = [0, 1, 2, 3]\n",
    "iteration = 0\n",
    "\n",
    "# initializing q values\n",
    "q = np.zeros((env.m, env.n, len(env.actions)))\n",
    "e = np.zeros((env.m, env.n, len(env.actions)))\n",
    "\n",
    "discount_factor = float(input(\"Enter the discount factor: \"))  # gamma\n",
    "update_factor = float(input(\"Enter the update factor: \"))  # alpha\n",
    "lam = float(input(\"Enter lambda, the factor that decays the eligibility traces: \"))\n",
    "\n",
    "episodes = int(input(\"How many episodes? \"))\n",
    "\n",
    "\n",
    "def get_action(rand_or_greedy, m, n):\n",
    "    if rand_or_greedy == 0:  # random\n",
    "        return random.choice(actions)\n",
    "\n",
    "    else:  # greedy\n",
    "        return np.argmax(q[m][n])\n",
    "\n",
    "\n",
    "steps = []\n",
    "for episode in range(episodes):\n",
    "    # defining epsilon: used to implement GLIE\n",
    "    # 0 means random, 1 means greedy action will be chosen.\n",
    "    # So, 0 -> epsilon 1 -> (1 - epsilon)\n",
    "\n",
    "    e = np.zeros((env.m, env.n, len(env.actions)))\n",
    "\n",
    "    epsilon = 1 / (episode + 1)\n",
    "\n",
    "    current_state = env.starting_point\n",
    "\n",
    "    random_or_greedy = np.random.choice(np.arange(0, 2), p=[epsilon, (1 - epsilon)])\n",
    "    action = get_action(random_or_greedy, current_state[0], current_state[1])\n",
    "    step_count = 0\n",
    "\n",
    "    while True:\n",
    "        reward, next_state, terminal_state = env.take_action(current_state[0], current_state[1], action)\n",
    "\n",
    "        e[current_state[0]][current_state[1]][action] += 1\n",
    "\n",
    "        random_or_greedy = np.random.choice(np.arange(0, 2), p=[epsilon, (1 - epsilon)])\n",
    "        action_dash = get_action(random_or_greedy, next_state[0], next_state[1])\n",
    "\n",
    "        delta = reward + discount_factor * q[next_state[0]][next_state[1]][action_dash] - q[current_state[0]][current_state[1]][action]\n",
    "\n",
    "        for i in range(env.m):\n",
    "            for j in range(env.n):\n",
    "                for k in range(len(env.actions)):\n",
    "                    q[i][j][k] += update_factor * delta * e[i][j][k]\n",
    "                    e[i][j][k] *= (discount_factor * lam)\n",
    "\n",
    "        current_state = next_state\n",
    "        action = action_dash\n",
    "        step_count += 1\n",
    "\n",
    "        if terminal_state:\n",
    "            steps.append(step_count)\n",
    "            break\n",
    "\n",
    "# POLICY:\n",
    "print(\"Policy: \")\n",
    "current_state = env.starting_point\n",
    "while True:\n",
    "    action = np.argmax(q[current_state[0]][current_state[1]])\n",
    "    # print(current_state)\n",
    "    if action == 0:\n",
    "        print(\"up\")\n",
    "    elif action == 1:\n",
    "        print(\"down\")\n",
    "    elif action == 2:\n",
    "        print(\"right\")\n",
    "    else:\n",
    "        print(\"left\")\n",
    "\n",
    "    reward, next_state, terminal_state = env.take_action(current_state[0], current_state[1], action)\n",
    "    current_state = next_state\n",
    "\n",
    "    if terminal_state:\n",
    "        break\n",
    "\n",
    "plt.plot(range(episodes), steps)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Steps taken to complete the episode\")\n",
    "plt.title(\"Steps taken per episode\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "<p>Once the agent finds the path with shortest path, the number of iterations required to complete the episode and reach the goal sharply</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
